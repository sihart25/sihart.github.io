```
{{ site.remote.bash_shebang }}
{{ site.sched.comment }} {{ site.sched.flag.name }} parallel-pi
{{ site.sched.comment }} {{ site.sched.flag.queue }} {{ site.sched.queue.testing }}

# Define, how many nodes you need. Here, we ask for 1 node.
# CPU: Sulis is configured such that SLURM refers to each processor core as a CPU. 
# Each EPYC processor contains 64 processor cores, and
# hence there are 128 CPUs total per node.
#  -N, --nodes=<n> Request that n nodes be allocated to this job
{{ site.sched.comment }} -N 1

# -n, --tasks-per-node= 4
{{ site.sched.comment }} -n 4
# we can request up to 3850 MB of RAM, which is the total amount of RAM available to user jobs
# in compute node, divided by the number of CPUs in the node. 
# i.e. 512 GB DDR4-3200 RAM per node ~> 4 GB RAM per core less over head for OS

# here we request mem per node 3GB 
{{ site.sched.comment }} --mem=3G

# Load the computing environment we need
module purge
module load GCCcore/11.2.0 OpenMPI/4.1.1 SciPy-bundle/2021.10

# Execute the task
srun python pi.py 100000000
```
{: .language-bash}
